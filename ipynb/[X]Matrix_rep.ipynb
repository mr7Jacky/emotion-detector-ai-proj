{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Conv2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout, Conv1D,Input,MaxPooling1D,Flatten\n",
    "import re\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 400000\n",
    "# Max number of words in each dialogue.\n",
    "MAX_SEQUENCE_LENGTH = 180\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Database/ISEAR/isear_databank.csv\")\n",
    "data = data[['Field1', 'SIT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('?', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"?\"\n",
    "text = nltk.word_tokenize(sentence)\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field1</th>\n",
       "      <th>SIT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joy</td>\n",
       "      <td>[period, falling, love, time, met, especially,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>[was, involved, traffic, accident]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>[was, driving, home, several, days, hard, work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sadness</td>\n",
       "      <td>[lost, person, meant, most, to]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>[time, knocked, deer, down, sight, animal, inj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>shame</td>\n",
       "      <td>[did, not, speak, truth]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>guilt</td>\n",
       "      <td>[caused, problems, somebody, could, not, keep,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>joy</td>\n",
       "      <td>[got, letter, offering, job, had, applied]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fear</td>\n",
       "      <td>[was, going, home, alone, night, man, came, up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>anger</td>\n",
       "      <td>[was, talking, to, party, first, time, long, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Field1                                                SIT\n",
       "0      joy  [period, falling, love, time, met, especially,...\n",
       "1     fear                 [was, involved, traffic, accident]\n",
       "2    anger  [was, driving, home, several, days, hard, work...\n",
       "3  sadness                    [lost, person, meant, most, to]\n",
       "4  disgust  [time, knocked, deer, down, sight, animal, inj...\n",
       "5    shame                           [did, not, speak, truth]\n",
       "6    guilt  [caused, problems, somebody, could, not, keep,...\n",
       "7      joy         [got, letter, offering, job, had, applied]\n",
       "8     fear  [was, going, home, alone, night, man, came, up...\n",
       "9    anger  [was, talking, to, party, first, time, long, w..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eliminator = ['CC', 'CD', 'DT', 'EX', 'IN', 'LS', 'NNP', 'PDT', 'POS', 'PRP', 'PRP$', 'UH', 'WDT', 'WP', 'WRB','SYM','.']\n",
    "sym = list('!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~รก\\n')\n",
    "for i in range(len(data['SIT'])):\n",
    "    cur_tok = nltk.word_tokenize(data['SIT'][i])\n",
    "    cur_tok = nltk.pos_tag(cur_tok)\n",
    "    rest = []\n",
    "    for word, t in cur_tok :\n",
    "        if not (t in eliminator or word in sym):\n",
    "            rest.append(word)\n",
    "    data['SIT'][i] = rest\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8583\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 10000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~รก\\n', lower=True)\n",
    "tokenizer.fit_on_texts(data['SIT'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (7666, 180)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(data['SIT'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covert to Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1379880.0\n"
     ]
    }
   ],
   "source": [
    "Z = np.zeros((len(X), len(word_index)))\n",
    "for i in range(len(X)):\n",
    "    seq = np.zeros(len(word_index))\n",
    "    for j in X[i]:\n",
    "        seq[j-1] += 1\n",
    "    Z[i] = seq\n",
    "print(np.sum(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7666, 8583, 1)\n"
     ]
    }
   ],
   "source": [
    "Z = np.expand_dims(Z,2)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (7666, 7) [0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "#TODO: apply the column for data Y\n",
    "Y = pd.get_dummies(data[ 'Field1']).values\n",
    "print('Shape of label tensor:', Y.shape, Y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.array([np.where(i == 1)[0][0] for i in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5366, 8583, 1) (5366,)\n",
      "(2300, 8583, 1) (2300,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(Z,K, test_size = 0.30, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start DL stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "stop = [EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)]\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, shuffle=True, batch_size=batch_size,validation_split=0.1,callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print(accr)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: actually use the trained model to experiment somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras import Model\n",
    "from keras.layers import Dense, Bidirectional, Embedding, LSTM, SpatialDropout1D, Input, Concatenate, GaussianNoise\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout, Flatten\n",
    "import os\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "\"\"\"\n",
    "Import scripts\n",
    "\"\"\"\n",
    "# Append the source code directory in to file\n",
    "sys.path.append(\"../src\") \n",
    "from preprocessing import *\n",
    "from embedding import *\n",
    "from model_builder import *\n",
    "\n",
    "\"\"\"\n",
    "Define Constant\n",
    "\"\"\"\n",
    "# Max number of words in each dialogue.\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "PLOT = False\n",
    "SIMPLE_CATEGORY = True\n",
    "CATEGORY_NUM = 5000\n",
    "\n",
    "\"\"\"\n",
    "Read data and preprocess it\n",
    "\"\"\"\n",
    "data = pd.read_csv('../Dataset/Tweets.csv')\n",
    "text_col = 'content'\n",
    "data = preprocess_text(data,text_col)\n",
    "# Shuffle data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\"\"\"\n",
    "Convert into 3 categories\n",
    "\"\"\"\n",
    "if SIMPLE_CATEGORY:\n",
    "    # Categorize into 3 categories\n",
    "    data.loc[data['sentiment'] == 'anger'] = 'negative'\n",
    "    data.loc[data['sentiment'] == 'hate'] = 'negative'\n",
    "    data.loc[data['sentiment'] == 'worry'] = 'negative'\n",
    "    data.loc[data['sentiment'] == 'sadness'] = 'negative'\n",
    "    data.loc[data['sentiment'] == 'boredom'] = 'negative'\n",
    "    data.loc[data['sentiment'] == 'relief'] = 'positive'\n",
    "    data.loc[data['sentiment'] == 'happiness'] = 'positive'\n",
    "    data.loc[data['sentiment'] == 'love'] = 'positive'\n",
    "    data.loc[data['sentiment'] == 'enthusiasm'] = 'positive'\n",
    "    data.loc[data['sentiment'] == 'surprise'] = 'positive'\n",
    "    data.loc[data['sentiment'] == 'fun'] = 'positive'\n",
    "    data.loc[data['sentiment'] == 'empty'] = 'neutral'\n",
    "    # divide into 3 data to maintain a evenly distributed dataset\n",
    "    neutral_data = data.loc[data['sentiment'] == 'neutral']\n",
    "    negative_data = data.loc[data['sentiment'] == 'negative']\n",
    "    positive_data = data.loc[data['sentiment'] == 'positive']\n",
    "    # Obtain 5000 from each category\n",
    "    data = pd.concat([neutral_data.sample(n=CATEGORY_NUM),\n",
    "                      negative_data.sample(n=CATEGORY_NUM),\n",
    "                      positive_data.sample(n=CATEGORY_NUM)])\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Create Embedding Layer\n",
    "\"\"\"\n",
    "embeddings, dim = get_embeddings(\"../glove/glove.6B.100d.txt\")\n",
    "tokenizer = get_tokenizer([' '.join(list(embeddings.keys()))])\n",
    "embedding_matrix = get_embedding_matrix(embeddings, tokenizer.word_index, dim)\n",
    "\n",
    "\"\"\"\n",
    "Create training and testing sets\n",
    "\"\"\"\n",
    "X = tokenizer.texts_to_sequences(data['content'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "Y = pd.get_dummies(data['sentiment']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.50, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  52.93333333333333\n"
     ]
    }
   ],
   "source": [
    "from SVM import svm_classify\n",
    "Z_test =[np.where(i==1)[0][0] for i in Y_test]\n",
    "Z_train =[np.where(i==1)[0][0] for i in Y_train]\n",
    "svm_classify(X_train, Z_train, X_test, Z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9444"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neutral_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
